{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "802e1346",
   "metadata": {},
   "source": [
    "### **Rhyme Analysis of Virgil's *The Æneid* in English translation** \n",
    "#### Bronwyn Bowles-King\n",
    "bronwynbowlesking@gmail.com\n",
    "\n",
    "This code is described in two articles published on Medium. See: https://medium.com/@bronwynbowlesking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b66f1f3",
   "metadata": {},
   "source": [
    "### **Part 1**\n",
    "#### Step 0: Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2905d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import csv\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import cmudict\n",
    "import ast\n",
    "from itertools import islice\n",
    "from wordcloud import WordCloud\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eca20bc",
   "metadata": {},
   "source": [
    "#### Step 1: Define functions to clean and prepare the text file \n",
    "\n",
    "1.1 Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b2692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_line(line):\n",
    "    # Normalise Unicode (NFKC)\n",
    "    line = unicodedata.normalize('NFKC', line.strip())\n",
    "    \n",
    "    # Replace Æ and æ with ae \n",
    "    line = re.sub(r'[Ææ]', 'ae', line, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Replace problematic symbols and split hyphenated words into two\n",
    "    line = re.sub(r'[\\u2010-\\u2015\\u2212]', '-', line)\n",
    "    line = re.sub(r'(?<=\\w)-(?=\\w)', ' ', line)\n",
    "    \n",
    "    # Remove any remaining non-ASCII characters\n",
    "    line = line.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # Remove punctuation, digits, and special dashes\n",
    "    line = re.sub(\n",
    "        fr\"[{re.escape(string.punctuation + string.digits)}\\u2010-\\u2015]\", \n",
    "        '', \n",
    "        line\n",
    "    )\n",
    "    \n",
    "    # Collapse whitespace, lowercase all text, and run a final strip for trailing space\n",
    "    return re.sub(r'\\s+', ' ', line).lower().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e425a2",
   "metadata": {},
   "source": [
    "1.2 Function to split text into books and store this as metadata for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9487d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_poem(file_path):\n",
    "    books = []\n",
    "    current_book = {'name': '', 'start': 0, 'end': 0}\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            cleaned = clean_line(line)\n",
    "            original = line.strip()\n",
    "            \n",
    "            if original.lower().startswith('book'):\n",
    "                if current_book['name']:\n",
    "                    current_book['end'] = len(cleaned_lines) - 1\n",
    "                    books.append(current_book)\n",
    "                current_book = {'name': original, 'start': len(cleaned_lines)}\n",
    "            \n",
    "            if cleaned:\n",
    "                cleaned_lines.append(cleaned)\n",
    "    \n",
    "    if current_book['name']:\n",
    "        current_book['end'] = len(cleaned_lines) - 1\n",
    "        books.append(current_book)\n",
    "    \n",
    "    return cleaned_lines, books"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d6df6a",
   "metadata": {},
   "source": [
    "1.3 Run the preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08415438",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'your/file/pathway/The_Aeneid.txt'\n",
    "\n",
    "cleaned_lines, books = preprocess_poem(file_path)\n",
    "\n",
    "# Save cleaned lines to a new file\n",
    "output_file_path = r'cleaned_Aeneid.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\\n\".join(cleaned_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27983808",
   "metadata": {},
   "source": [
    "#### Step 2: Define the functions needed for rhyme analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7ec73b",
   "metadata": {},
   "source": [
    "\n",
    "2.1 Define a function to create a pronunciation dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f88896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CMU Pronouncing Dictionary\n",
    "pronunciation_dict = cmudict.dict()\n",
    "\n",
    "def get_all_pronunciations(word):\n",
    "    # Returns list of lists of pronunciations for a word and if a word is not found in the dictionary, it returns an empty list\n",
    "    return pronunciation_dict.get(word.lower().strip('.,!?;:\"\\'()-'), [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171308b6",
   "metadata": {},
   "source": [
    "2.1.1 Check if the dictionary has loaded and for target words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9041a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"mind\"]\n",
    "for w in words:\n",
    "    print(f\"{w}: {pronunciation_dict.get(w.lower())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d114ae5e",
   "metadata": {},
   "source": [
    "2.2 Extract the target rhyming portion of phonetic representations for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78937a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rhyme_tail(pron):\n",
    "    vowels = {'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'EH', 'ER', 'EY',\n",
    "              'IH', 'IY', 'OW', 'OY', 'UH', 'UW'}\n",
    "    for i in reversed(range(len(pron))):  # Remove stress markers \n",
    "        if pron[i][:2] in vowels:\n",
    "            return pron[i:]\n",
    "    return pron  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33c7f75",
   "metadata": {},
   "source": [
    "2.3 Detect perfect and near rhymes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9dca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if unique pairs of words rhyme perfectly\n",
    "\n",
    "def is_rhyme(word1, word2, pron_cache):\n",
    "    pron1_list = pron_cache.get(word1, [])\n",
    "    pron2_list = pron_cache.get(word2, [])\n",
    "    for pron1 in pron1_list:\n",
    "        tail1 = extract_rhyme_tail(pron1)\n",
    "        for pron2 in pron2_list:\n",
    "            tail2 = extract_rhyme_tail(pron2)\n",
    "            if tail1 == tail2:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f6a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect near rhymes\n",
    "\n",
    "def is_near_rhyme(word1, word2, pron_cache):\n",
    "    pron1_list = pron_cache.get(word1, [])\n",
    "    pron2_list = pron_cache.get(word2, [])\n",
    "    \n",
    "    # Check stress in any pronunciation\n",
    "    stress_match = any(\n",
    "        any('1' in phoneme for phoneme in pron1) and \n",
    "        any('1' in phoneme for phoneme in pron2)\n",
    "        for pron1 in pron1_list \n",
    "        for pron2 in pron2_list\n",
    "    )\n",
    "    \n",
    "    # Check final 2 phonemes\n",
    "    phoneme_match = any(\n",
    "        p1[-2:] == p2[-2:] \n",
    "        for p1 in pron1_list \n",
    "        for p2 in pron2_list \n",
    "        if len(p1)>=2 and len(p2)>=2\n",
    "    )\n",
    "    \n",
    "    return stress_match and phoneme_match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea284bd",
   "metadata": {},
   "source": [
    "2.4 Extract unique line pairs and construct a rhyme-checking structure to handle the Spenserian stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bab2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_pairs(indices):\n",
    "    return [(i, j) for idx, i in enumerate(indices) for j in indices[idx+1:]]\n",
    "\n",
    "spenserian_stanza_length = 9\n",
    "a_group = [0, 2] # Rhyme groups by line index starting with 0\n",
    "b_group = [1, 3, 4, 6]\n",
    "c_group = [5, 7, 8]\n",
    "\n",
    "# Check for rhymes within stanzas based on rhyme scheme *ababbcbcc*\n",
    "spenserian_pairs = all_pairs(a_group) + all_pairs(b_group) + all_pairs(c_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb50624",
   "metadata": {},
   "source": [
    "2.5 Group lines by book name and store as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a6bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the cleaned text \n",
    "with open('cleaned_Aeneid.txt', 'r', encoding='utf-8'): \n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Group lines by book \n",
    "books = {}\n",
    "current_book = None\n",
    "for line in lines:\n",
    "    if line.lower().startswith('book'):\n",
    "        current_book = line\n",
    "        books[current_book] = []\n",
    "    elif current_book:\n",
    "        books[current_book].append(line)\n",
    "\n",
    "print(f\"Books: {list(books.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a983a6",
   "metadata": {},
   "source": [
    "2.6 View the number of lines per book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21435338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate no. of lines per book\n",
    "book_names = list(books.keys())\n",
    "lines_per_book = [len(lines) for lines in books.values()]\n",
    "average_lines = sum(lines_per_book) / len(lines_per_book)\n",
    "\n",
    "# Print results. Every result should be divisible by 9 (stanza length)\n",
    "print(\"Detailed line counts\")\n",
    "for i, book_name in enumerate(book_names):\n",
    "    print(f\"{book_name}: {lines_per_book[i]} lines\") \n",
    "\n",
    "print(f\"\\nAverage lines per book: {average_lines:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1fd1de",
   "metadata": {},
   "source": [
    "2.7 Build a global pronunciation cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8cb11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_pron_cache = {}\n",
    "for book_lines in books.values():\n",
    "    for line in book_lines:\n",
    "        if line.strip():\n",
    "            last_word = line.split()[-1].lower().strip('.,!?;:\"\\'()-')\n",
    "            if last_word not in global_pron_cache:\n",
    "                global_pron_cache[last_word] = get_all_pronunciations(last_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de40958e",
   "metadata": {},
   "source": [
    "2.7.1 Check if the code is working properly and for granular data with these tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269961be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first 5 items of global_pron_cache\n",
    "\n",
    "for k, v in islice(global_pron_cache.items(), 5):\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89de4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many words have more than one pronunciation to ensure multiple representations loaded\n",
    "\n",
    "multi_pron_count = sum(1 for prons in global_pron_cache.values() if len(prons) > 1)\n",
    "print(f\"Number of words with more than one pronunciation: {multi_pron_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05428ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for target words in the dictionary\n",
    "\n",
    "word = \"strong\"  \n",
    "if word in global_pron_cache:\n",
    "    print(f\"Phonetic representation/s for '{word}': {global_pron_cache[word]}\")\n",
    "else:\n",
    "    print(f\"'{word}' not found in global_pron_cache.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe47e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any stanzas were excluded from the analysis\n",
    "\n",
    "incomplete_stanzas = []\n",
    "for book_name, book_lines in books.items():\n",
    "    for stanza_start in range(0, len(book_lines), spenserian_stanza_length):\n",
    "        stanza = book_lines[stanza_start:stanza_start+spenserian_stanza_length]\n",
    "        if len(stanza) < spenserian_stanza_length:\n",
    "            incomplete_stanzas.append({\n",
    "                'book': book_name,\n",
    "                'stanza_start_line': stanza_start + 1,  \n",
    "                'lines_in_stanza': len(stanza)\n",
    "            })\n",
    "\n",
    "print(f\"Incomplete stanzas: {incomplete_stanzas[:10]}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef86efc1",
   "metadata": {},
   "source": [
    "2.7.2 Save pronunication cache for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ea3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a sample of global_pron_cache to inspect the structure\n",
    "\n",
    "with open(\"sample_global_pron_cache.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for k, v in islice(global_pron_cache.items(), 20):\n",
    "        writer.writerow([k, v])\n",
    "\n",
    "# Save global pronunciation cache as .pkl file\n",
    "\n",
    "with open('global_pron_cache.pkl', 'wb') as f:\n",
    "    pickle.dump(global_pron_cache, f)\n",
    "\n",
    "# Load .pkl file later\n",
    "\n",
    "with open('global_pron_cache.pkl', 'rb') as f:\n",
    "   global_pron_cache = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a5a791",
   "metadata": {},
   "source": [
    "### **Part 2**\n",
    "#### Step 3: Run the rhyme detection process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51690e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for book_name, book_lines in books.items():\n",
    "    for stanza_start in range(0, len(book_lines), spenserian_stanza_length):\n",
    "        stanza = book_lines[stanza_start:stanza_start+spenserian_stanza_length]\n",
    "        if len(stanza) < spenserian_stanza_length:\n",
    "            continue  \n",
    "        \n",
    "        last_words = [\n",
    "            line.split()[-1].lower().strip('.,!?;:\"\\'()-') \n",
    "            for line in stanza\n",
    "        ]\n",
    "        pron_cache = {word: global_pron_cache.get(word, []) for word in last_words}\n",
    "        \n",
    "        for i, j in spenserian_pairs:\n",
    "            word1 = last_words[i]\n",
    "            word2 = last_words[j]\n",
    "            # Check for perfect rhyme\n",
    "            perfect_rhyme = is_rhyme(word1, word2, pron_cache)\n",
    "            # Check for near rhyme if not perfect\n",
    "            near_rhyme = not perfect_rhyme and is_near_rhyme(word1, word2, pron_cache)\n",
    "            if perfect_rhyme or near_rhyme:\n",
    "                results.append({\n",
    "                    'book': book_name,\n",
    "                    'line1': stanza_start+i+1,\n",
    "                    'line2': stanza_start+j+1,\n",
    "                    'word1': word1,\n",
    "                    'word2': word2,\n",
    "                    'rhyme_type': 'perfect' if perfect_rhyme else 'near',\n",
    "                    'text1': stanza[i],\n",
    "                    'text2': stanza[j]\n",
    "                })\n",
    "\n",
    "#  Save and display results \n",
    "rhyme_df = pd.DataFrame(results)\n",
    "print(rhyme_df.head())\n",
    "\n",
    "rhyme_df.to_csv('cmu_rhyme_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee790d67",
   "metadata": {},
   "source": [
    "#### Step 4: Identify lines missed in the rhyme analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b332033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_lines = set()\n",
    "for _, row in rhyme_df.iterrows():\n",
    "    existing_lines.add((row['book'], row['line1']))\n",
    "    existing_lines.add((row['book'], row['line2']))\n",
    "\n",
    "\n",
    "missing_lines = []\n",
    "for book_name, book_lines in books.items():\n",
    "    for idx, line in enumerate(book_lines, start=1): \n",
    "        if (book_name, idx) not in existing_lines:\n",
    "            missing_lines.append({\n",
    "                'book': book_name,\n",
    "                'line_number': idx,\n",
    "                'text': line\n",
    "            })\n",
    "\n",
    "print(f\"Missing lines (global_pron): {len(missing_lines)}.\")\n",
    "\n",
    "# Save to CSV\n",
    "missing_df = pd.DataFrame(missing_lines)\n",
    "missing_df.to_csv('missing_lines.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2354c",
   "metadata": {},
   "source": [
    "#### Step 5: Check which words have no pronunciation available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = [k for k, v in global_pron_cache.items() if not v]\n",
    "print(f\"Words with no pronunciation (global_pron_cache): {len(empty)}\")\n",
    "print(\"First five words with no pronunciation (global_pron_cache):\", empty[:5])\n",
    "\n",
    "with open('words_no_pronunciation.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['word'])\n",
    "    for word in empty:\n",
    "        writer.writerow([word])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6312dbe5",
   "metadata": {},
   "source": [
    "#### Step 6: Create a list of phonetic representations for missing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373d8bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom pronunciations CSV\n",
    "custom_pron_path = 'your/file/pathway/missing_phonetics.csv'\n",
    "custom_pron = {}\n",
    "\n",
    "with open(custom_pron_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        word = row['word'].strip().lower()\n",
    "        try:\n",
    "            pron = ast.literal_eval(row['phonemes'])  \n",
    "        except Exception:\n",
    "            pron = []\n",
    "        custom_pron[word] = pron\n",
    "\n",
    "# Copy, rename and update cache with custom pronunciations\n",
    "pron_cache_all = global_pron_cache.copy()\n",
    "pron_cache_all.update(custom_pron)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218ff6c9",
   "metadata": {},
   "source": [
    "6.1 Check if the join between global_pron_cache and custom_pron was successful by searching for a target word and showing words with no pronunciations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b731fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"oer\"  \n",
    "if word in pron_cache_all:\n",
    "    print(f\"Phonetic representation/s for '{word}': {pron_cache_all[word]}.\")\n",
    "else:\n",
    "    print(f\"'{word}' not found in pron_cache_all.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae59e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = [k for k, v in pron_cache_all.items() if not v]\n",
    "print(f\"Words with no pronunciation (pron_cache_all): {len(empty)}\")\n",
    "print(\"First five words with no pronunciation (pron_cache_all):\", empty[:5])\n",
    "\n",
    "with open('words_no_pronunciation.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['word'])\n",
    "    for word in empty:\n",
    "        writer.writerow([word])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ae9447",
   "metadata": {},
   "source": [
    "#### Step 7: Re-run the rhyme analysis and check for missing lines again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fa9bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for book_name, book_lines in books.items():\n",
    "    for stanza_start in range(0, len(book_lines), spenserian_stanza_length):\n",
    "        stanza = book_lines[stanza_start:stanza_start+spenserian_stanza_length]\n",
    "        if len(stanza) < spenserian_stanza_length:\n",
    "            continue  \n",
    "        \n",
    "        last_words = [\n",
    "            line.split()[-1].lower().strip('.,!?;:\"\\'()-') \n",
    "            for line in stanza\n",
    "        ]\n",
    "        pron_cache = {word: pron_cache_all.get(word, []) for word in last_words}\n",
    "        \n",
    "        for i, j in spenserian_pairs:\n",
    "            word1 = last_words[i]\n",
    "            word2 = last_words[j]\n",
    "            perfect_rhyme = is_rhyme(word1, word2, pron_cache)\n",
    "            near_rhyme = not perfect_rhyme and is_near_rhyme(word1, word2, pron_cache)\n",
    "            if perfect_rhyme or near_rhyme:\n",
    "                results.append({\n",
    "                    'book': book_name,\n",
    "                    'line1': stanza_start+i+1,\n",
    "                    'line2': stanza_start+j+1,\n",
    "                    'word1': word1,\n",
    "                    'word2': word2,\n",
    "                    'rhyme_type': 'perfect' if perfect_rhyme else 'near',\n",
    "                    'text1': stanza[i],\n",
    "                    'text2': stanza[j]\n",
    "                })\n",
    "\n",
    "#  Save and display results \n",
    "rhyme_df = pd.DataFrame(results)\n",
    "print(rhyme_df.head())\n",
    "rhyme_df.to_csv('custom_rhyme_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e13b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_lines = set()\n",
    "for _, row in rhyme_df.iterrows():\n",
    "    existing_lines.add((row['book'], row['line1']))\n",
    "    existing_lines.add((row['book'], row['line2']))\n",
    "\n",
    "\n",
    "missing_lines = []\n",
    "for book_name, book_lines in books.items():\n",
    "    for idx, line in enumerate(book_lines, start=1): \n",
    "        if (book_name, idx) not in existing_lines:\n",
    "            missing_lines.append({\n",
    "                'book': book_name,\n",
    "                'line_number': idx,\n",
    "                'text': line\n",
    "            })\n",
    "\n",
    "print(f\"Missing lines (pron_cache_all): {len(missing_lines)}.\")\n",
    "\n",
    "# Save to CSV\n",
    "missing_df = pd.DataFrame(missing_lines)\n",
    "missing_df.to_csv('missing_lines2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d896e0",
   "metadata": {},
   "source": [
    "7.1 Save new pronunication cache for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90ce41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a sample of pron_cache_all to inspect the structure\n",
    "with open(\"sample_pron_cache_all.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for k, v in islice(pron_cache_all.items(), 20):\n",
    "        writer.writerow([k, v])\n",
    "\n",
    "# Save pron_cache_all as .pkl file\n",
    "with open('pron_cache_all.pkl', 'wb') as f:\n",
    "     pickle.dump(pron_cache_all, f)\n",
    "\n",
    "# Load .pkl file if needed later\n",
    "with open('pron_cache_all.pkl', 'rb') as f:\n",
    "    pron_cache_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b50a970",
   "metadata": {},
   "source": [
    "#### Step 8: Calculate the consistency of rhyme detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85acfc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rhyme pairs found: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a32688c",
   "metadata": {},
   "source": [
    "#### Step 9: Create and save a list of final undetected words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d01721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all last words from all lines\n",
    "all_last_words = set()\n",
    "for book_lines in books.values():\n",
    "    for line in book_lines:\n",
    "        if line.strip():\n",
    "            last_word = line.split()[-1].lower().strip('.,!?;:\"\\'()-')\n",
    "            all_last_words.add(last_word)\n",
    "\n",
    "# Collect all words that appear in any rhyme pair\n",
    "rhymed_words = set(rhyme_df['word1']).union(set(rhyme_df['word2']))\n",
    "\n",
    "# Find words that never rhyme\n",
    "unrhymed_words = sorted(all_last_words - rhymed_words)\n",
    "\n",
    "print(f\"Number of last words not detected as rhyming: {len(unrhymed_words)}\")\n",
    "\n",
    "# Save to CSV\n",
    "unrhymed_path = 'final_missing_words.csv'\n",
    "with open(unrhymed_path, 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['word'])\n",
    "    for word in unrhymed_words:\n",
    "        writer.writerow([word])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd317d1",
   "metadata": {},
   "source": [
    "#### Step 10: Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a74a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure rhyme_df is defined\n",
    "if 'rhyme_df' not in globals():\n",
    "    rhyme_df = pd.DataFrame(results)\n",
    "\n",
    "# Count frequency of rhymed words by type\n",
    "perfect_counts = {}\n",
    "near_counts = {}\n",
    "\n",
    "for _, row in rhyme_df.iterrows():\n",
    "    if row['rhyme_type'] == 'perfect':\n",
    "        perfect_counts[row['word1']] = perfect_counts.get(row['word1'], 0) + 1\n",
    "        perfect_counts[row['word2']] = perfect_counts.get(row['word2'], 0) + 1\n",
    "    elif row['rhyme_type'] == 'near':\n",
    "        near_counts[row['word1']] = near_counts.get(row['word1'], 0) + 1\n",
    "        near_counts[row['word2']] = near_counts.get(row['word2'], 0) + 1\n",
    "\n",
    "# Generate word clouds\n",
    "wc_perfect = WordCloud(width=800, height=400, max_words=40, background_color='white', colormap='viridis', font_path='C:/Windows/Fonts/GARABD.ttf').generate_from_frequencies(perfect_counts) # GARABD is Garamond bold from Windows default fonts\n",
    "wc_near = WordCloud(width=800, height=400, max_words=40, background_color='white', colormap='plasma', font_path='C:/Windows/Fonts/GARABD.ttf').generate_from_frequencies(near_counts)\n",
    "\n",
    "# Plot side by side\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(wc_perfect, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Perfect Rhymes\\n')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(wc_near, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Near Rhymes\\n')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf5b723",
   "metadata": {},
   "source": [
    "#### Step 11: Provide a list of the most frequent end-rhyming words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873684f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all last words from all lines in all books\n",
    "all_last_words = []\n",
    "for book_lines in books.values():\n",
    "    for line in book_lines:\n",
    "        if line.strip():\n",
    "            last_word = line.split()[-1].lower().strip('.,!?;:\"\\'()-')\n",
    "            all_last_words.append(last_word)\n",
    "\n",
    "# Count frequency of each last word\n",
    "last_word_counts = Counter(all_last_words)\n",
    "\n",
    "# Sort by frequency (descending)\n",
    "sorted_last_words = last_word_counts.most_common()\n",
    "\n",
    "# Print top 10 most frequent last words\n",
    "print(\"Top 10 most frequent last words:\")\n",
    "for word, count in sorted_last_words[:10]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save to CSV\n",
    "freq_path = 'last_word_frequencies.csv'\n",
    "with open(freq_path, 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['word', 'frequency'])\n",
    "    for word, count in sorted_last_words:\n",
    "        writer.writerow([word, count])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
